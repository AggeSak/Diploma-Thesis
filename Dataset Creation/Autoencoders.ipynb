{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I used a convolutional autoencoder and collected the representation of the bottleneck to create rgb images.\n",
    "\n",
    "This code implements a convolutional autoencoder (CAE) designed to compress and reconstruct grayscale images of size 1024x1024. Here's a step-by-step breakdown:\n",
    "\n",
    "Data Preprocessing:\n",
    "\n",
    "CustomPadCrop: Ensures images are padded (if smaller) or center-cropped (if larger) to the target size (1024x1024).\n",
    "\n",
    "ConditionalTransform: Dynamically applies resizing (for narrow images) or CustomPadCrop (for wider images) to standardize input dimensions.\n",
    "\n",
    "ImageDataset: Loads images from specified directories (malignant/benign tumor images), filters valid files, and applies transformations.\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "Encoder: Downsamples images via convolutional layers (Conv2d) and max pooling:\n",
    "\n",
    "Input: 1x1024x1024 → 16x512x512 → 3x256x256 (bottleneck).\n",
    "\n",
    "Decoder: Upsamples the bottleneck back to the original size using transposed convolutions (ConvTranspose2d):\n",
    "\n",
    "Bottleneck: 3x256x256 → 16x512x512 → 16x1024x1024 → 1x1024x1024 (sigmoid output).\n",
    "\n",
    "Training Setup:\n",
    "\n",
    "Loss Function: Uses Mean Squared Error (MSE) to measure reconstruction error.\n",
    "\n",
    "Optimizer: Adam optimizer with learning rate 1e-3 and weight decay 1e-5 for regularization.\n",
    "\n",
    "Data Pipeline: Batches images via DataLoader (batch size=32) for efficient training.\n",
    "\n",
    "The autoencoder learns to compress input images into a compact latent representation (3x256x256) and reconstruct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "class CustomPadCrop:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        if width < self.target_size[0] or height < self.target_size[1]:\n",
    "            padding_width = max((self.target_size[0] - width) // 2, 0)\n",
    "            padding_height = max((self.target_size[1] - height) // 2, 0)\n",
    "            padded_img = Image.new(\"L\", self.target_size, color=0)\n",
    "            padded_img.paste(img, (padding_width, padding_height))\n",
    "            return padded_img\n",
    "        elif width > self.target_size[0] or height > self.target_size[1]:\n",
    "            return transforms.functional.center_crop(img, self.target_size)\n",
    "        return img\n",
    "\n",
    "class ConditionalTransform:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "        self.resize_transform = transforms.Resize(self.target_size)\n",
    "        self.custom_transform = CustomPadCrop(self.target_size)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        width, _ = img.size\n",
    "        if width < self.target_size[0]:\n",
    "            # Use Resize to make it 1024x1024, aspect ratio might not be maintained\n",
    "            img = self.resize_transform(img)\n",
    "        else:\n",
    "            # Use custom pad crop\n",
    "            img = self.custom_transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dirs, transform=None):\n",
    "        self.root_dirs = root_dirs\n",
    "        self.transform = transform\n",
    "        self.file_paths = self.collect_file_paths()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.file_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('L')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def collect_file_paths(self):\n",
    "        file_paths = []\n",
    "        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff', '*.gif', '*.webp']\n",
    "        for root_dir in self.root_dirs:\n",
    "            for pattern in image_extensions:\n",
    "                files = glob.glob(os.path.join(root_dir, '**', pattern), recursive=True)\n",
    "                for file in files:\n",
    "                    if os.path.getsize(file) > 0:\n",
    "                        try:\n",
    "                            img = Image.open(file)\n",
    "                            file_paths.append(file)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Cannot open {file}: {e}\")\n",
    "        return file_paths\n",
    "\n",
    "# Define transformations\n",
    "target_size = (1024, 1024)\n",
    "transform = transforms.Compose([\n",
    "    ConditionalTransform(target_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define directories containing the images\n",
    "root_dirs = [\n",
    "    'path/to/my/image1',\n",
    "    'path/to/my/images2\n",
    "]\n",
    "\n",
    "# Create a custom dataset using the directories and transformation\n",
    "custom_dataset = ImageDataset(root_dirs, transform=transform)\n",
    "\n",
    "# Create a DataLoader to load the data in batches during training\n",
    "data_loader = torch.utils.data.DataLoader(dataset=custom_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),       # -> [N, 16, 1024, 1024]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                   # -> [N, 16, 512, 512]\n",
    "            nn.Conv2d(16, 3, 3, padding=1),       # -> [N, 3, 512, 512]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),                   # -> [N, 3, 256, 256]  (Bottleneck size)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3, 16, kernel_size=2, stride=2, padding=0, output_padding=0),  # -> [N, 16, 512, 512]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=2, stride=2, padding=0, output_padding=0), # -> [N, 16, 1024, 1024]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 1, kernel_size=1, stride=1, padding=0),  # -> [N, 1, 1024, 1024]\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for grayscale images\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Instantiate the model\n",
    "model = Autoencoder()\n",
    "\n",
    "# Verify model parameters\n",
    "params = list(model.parameters())\n",
    "print(len(params))  # Should be greater than 0\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#here we implement the autoencoders training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Disable the DecompressionBombError by setting the MAX_IMAGE_PIXELS to None\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Device setup (if not already done)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Ensure model is on the same device\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 3  # Adjust this as necessary\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Use tqdm to wrap the data loader for a progress bar\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        tepoch.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "        for data in tepoch:\n",
    "            # Move images to device (GPU/CPU)\n",
    "            data = data.to(device)  # Ensure data is on the same device as the model\n",
    "\n",
    "            # Forward pass: input -> autoencoder -> output\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()        # Backpropagation\n",
    "            optimizer.step()       # Update weights\n",
    "\n",
    "            # Accumulate loss and update progress bar description\n",
    "            running_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete\")\n",
    "# Save the model\n",
    "save_path = 'path/to/save'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
