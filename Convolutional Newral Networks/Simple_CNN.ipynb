{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Disable the DecompressionBombError by setting the MAX_IMAGE_PIXELS to None\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the path to the .npz file\n",
    "npz_file_path = path/to/your/npz/file'\n",
    "'In this project the npz files were very usefull but ofcource it can be the files that the images are stored directly here'\n",
    "\n",
    "# Check if the .npz file exists\n",
    "if not os.path.exists(npz_file_path):\n",
    "    print(\"NPZ file not found.\")\n",
    "    exit()  # Exit the program if the file is missing\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load(npz_file_path, allow_pickle=True)\n",
    "\n",
    "# Retrieve datasets\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"Number of training samples: {len(X_train)}\")\n",
    "print(f\"Number of validation samples: {len(X_val)}\")\n",
    "print(f\"Number of test samples: {len(X_test)}\")\n",
    "\n",
    "# Convert file paths from numpy arrays to lists\n",
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "\n",
    "\n",
    "class CustomResizeAndPadOrCrop:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "\n",
    "        # Check if the image needs to be resized and padded or cropped\n",
    "        if width < self.target_size[0] or height < self.target_size[1]:\n",
    "            # Calculate padding to center the image\n",
    "            padding_width = max((self.target_size[0] - width) // 2, 0)\n",
    "            padding_height = max((self.target_size[1] - height) // 2, 0)\n",
    "\n",
    "            # Create a new blank RGB image of target size\n",
    "            padded_img = Image.new(\"L\", self.target_size, color=0)  # Set color to black for RGB\n",
    "\n",
    "            # Paste the original image onto the blank image at the center\n",
    "            padded_img.paste(img, (padding_width, padding_height))\n",
    "\n",
    "            # Return the padded image\n",
    "            img = padded_img\n",
    "\n",
    "        elif width > self.target_size[0] or height > self.target_size[1]:\n",
    "            # Crop the image from the top-left corner\n",
    "            img = transforms.functional.crop(img, 0, 0, self.target_size[1], self.target_size[0])\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=25, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose  # Set verbose to True to enable printing\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    CustomResizeAndPadOrCrop((1024, 1024)),  # Adjust size as needed\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],  \n",
    "    # Normalize with ImageNet mean\n",
    "    # std=[0.229, 0.224, 0.225])  Normally this transformation help , although it didn't this time\n",
    "])\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 32\n",
    "train_dataset = ImageDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = ImageDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = ImageDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a Simple CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=15, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=15, out_channels=25, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        self.fc1 = nn.Linear(in_features=25 * 62 * 62, out_features=1000)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 25 * 62 * 62)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and move model to GPU\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "# Training Loop\n",
    "# Train the model\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    train_acc = correct_train / total_train\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_dataset)\n",
    "test_acc = test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_predictions.extend(predicted.cpu().tolist())\n",
    "        test_targets.extend(labels.cpu().tolist())\n",
    "\n",
    "precision = precision_score(test_targets, test_predictions)\n",
    "recall = recall_score(test_targets, test_predictions)\n",
    "f1 = f1_score(test_targets, test_predictions)\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(test_targets, test_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"          Predicted\")\n",
    "print(\"         0      1\")\n",
    "print(f\"Actual 0 {cm[0,0]}    {cm[0,1]}\")\n",
    "print(f\"Actual 1 {cm[1,0]}    {cm[1,1]}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = 'path/to/save'  #here put the path you want to save your mode\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved at {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
